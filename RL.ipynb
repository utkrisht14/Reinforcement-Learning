{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7744c0bd-715f-41a6-ac76-c04ce115e27a",
   "metadata": {},
   "source": [
    "## Stock trading using Reinforcement Learning (Q-learning) \n",
    "In this program we will make an agent to deal with stock market. For this we will use Q-learning alogrithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d1af1d07-3ef8-4c6c-9643-20ea1cefc9e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading the necessary libraries \n",
    "import numpy as np\n",
    "import gym\n",
    "from gym import spaces\n",
    "import random\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a3b39b9-d3ab-4a3a-bd9e-c8708508593f",
   "metadata": {},
   "source": [
    "We will now define the trading environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b9c46efd-1518-4630-9224-a98373e10f52",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TradingEnv(gym.Env):\n",
    "    def __init__(self,df):\n",
    "        super().__init__()\n",
    "        self.df = df\n",
    "        self.reward_range = (-np.inf,np.inf) # Reward ranges from negative infinity to positive infinity \n",
    "        self.action_space = spaces.Discrete(3) # 0: hold, 1: buy, 2: sell\n",
    "        self.observation_space = spaces.Box(low = -np.inf, high = np.inf, shape = (df.shape[1],), dtype= np.float32)\n",
    "\n",
    "        self.current_step = 0 \n",
    "        self.done = False\n",
    "        self.positions = [] # No positions hold in the beginning\n",
    "        self.current_price = 0\n",
    "        self.cash_in_hand = 1000 # Cash with what we allow to start the agent\n",
    "\n",
    "    def _next_observation(self):\n",
    "        return self.df.iloc[self.current_step].values\n",
    "\n",
    "    def step(self,action):\n",
    "        reward = 0 # Initialize reward\n",
    "        self.current_price = self.df.loc[self.current_step,\"stock_price\"]\n",
    "\n",
    "        # Buy action\n",
    "        if action == 1:\n",
    "            self.positions.append(self.current_price)\n",
    "            self.cash_in_hand -= self.current_price # Deduct the money of current stock \n",
    "\n",
    "        # Sell action\n",
    "        elif action==2 and len(self.positions)>0:\n",
    "            bought_price = self.positions.pop(0)\n",
    "            self.cash_in_hand += self.current_price\n",
    "            reward = self.current_price - bought_price\n",
    "\n",
    "        # Hold or invalid sell\n",
    "        else:\n",
    "            self.current_step +=1 \n",
    "            if self.current_step >= len(self.df): # If we have reached to the last of the data \n",
    "                self.done = True \n",
    "\n",
    "        info = {}\n",
    "        next_state = self._next_observation() if not self.done else np.zeros(self.df.shape[1])\n",
    "        return next_state, reward, self.done, info\n",
    "\n",
    "    # Define a function for reset\n",
    "    def reset(self):\n",
    "        self.current_step = 0 \n",
    "        self.done = False\n",
    "        self.positions = []\n",
    "        self.current_price = self.df.loc[self.current_step, \"stock_price\"]\n",
    "        self.cash_in_hand = 1000\n",
    "        return self._next_observation()\n",
    "\n",
    "    def render(self,mode=\"human\"):\n",
    "        profit = self.cash_in_hand - 1000 + sum([self.current_price - p for p in self.positions])\n",
    "        return f\"Step: {self.current_step}, Price: {self.current_price:.2f}, Positions: {len(self.positions)}, Profit: {profit:.2f}\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28f73e3e-b717-427e-ad70-02225ef436b4",
   "metadata": {},
   "source": [
    "Next we will define the Q-learning agent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "d5b9f7dd-d406-4cba-8cf4-289feb11cc2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class QLearningAgent:\n",
    "    def __init__(self,state_size, action_size, alpha=0.1, gamma=0.6, epsilon = 0.1):\n",
    "        self.state_size = state_size # State space size\n",
    "        self.action_size = action_size # Action space size\n",
    "        self.alpha = alpha # Learning rate \n",
    "        self.gamma = gamma # Discount \n",
    "        self.epsilon = epsilon # Extrapolation rate \n",
    "        self.q_table = np.zeros((state_size, action_size)) # Initializing Q table with zero \n",
    "\n",
    "    # Method to choose an action based on the current state \n",
    "    def choose_action(self,state_index):\n",
    "        # With probability epsilon, choose a random number\n",
    "        if random.uniform(0,1) < self.epsilon:\n",
    "            return random.choice(range(self.action_size))\n",
    "        # Otherwise, choose the action with the highest Q-value \n",
    "        else:\n",
    "            return np.argmax(self.q_table[state_index])\n",
    "\n",
    "   \n",
    "    # Method to update the Q-table based on agent's experience\n",
    "    def learn(self,state_index, action, reward, next_state_index, done):\n",
    "        \n",
    "        # Get the current Q-value for the state-action pair\n",
    "        old_value = self.q_table[state_index,action]\n",
    "        \n",
    "        # Get the maximum Q-value for the next state\n",
    "        next_max = np.max(self.q_table[next_state_index])\n",
    "\n",
    "        # Compute the new Q-value for the state-action pair\n",
    "        new_value = old_value + self.alpha * (reward + self.gamma * next_max *(1 - int(done)) - old_value)\n",
    "\n",
    "        # Update the Q-table with the new Q-value\n",
    "        self.q_table[state_index,action] = new_value"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbae49b0-860b-4536-a2ee-619cf056a3a8",
   "metadata": {},
   "source": [
    "Now we load the dataframe for which we will make the predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "a42e247e-39fe-425e-880b-f2a9e406b0b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the data file \n",
    "df = pd.read_csv(r\"C:\\Users\\utkri\\OneDrive\\Desktop\\VG_option_simulation.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9ff67eb-dd01-4548-a669-535ebe72529d",
   "metadata": {},
   "source": [
    "Now we initailze the environment and the agent. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "4ded8dba-ee34-42f5-99bb-6f429127390b",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = TradingEnv(df)\n",
    "state_size = env.observation_space.shape[0]\n",
    "action_size = env.action_space.n\n",
    "agent = QLearningAgent(state_size=state_size, action_size= action_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4119687-7229-4769-9dde-2e051729b1f8",
   "metadata": {},
   "source": [
    "Next we train the agent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "6d1c84b4-40af-4067-af33-ae00d230b736",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 1/100, Profit: -7.275957614183426e-12\n",
      "Episode: 2/100, Profit: -3.637978807091713e-12\n",
      "Episode: 3/100, Profit: -3.637978807091713e-12\n",
      "Episode: 4/100, Profit: -3.637978807091713e-12\n",
      "Episode: 5/100, Profit: 9.094947017729282e-13\n",
      "Episode: 6/100, Profit: 0.0\n",
      "Episode: 7/100, Profit: -9.094947017729282e-13\n",
      "Episode: 8/100, Profit: 0.0\n",
      "Episode: 9/100, Profit: -3.637978807091713e-12\n",
      "Episode: 10/100, Profit: -7.275957614183426e-12\n",
      "Episode: 11/100, Profit: -3.637978807091713e-12\n",
      "Episode: 12/100, Profit: 0.0\n",
      "Episode: 13/100, Profit: 7.275957614183426e-12\n",
      "Episode: 14/100, Profit: -7.275957614183426e-12\n",
      "Episode: 15/100, Profit: -3.637978807091713e-12\n",
      "Episode: 16/100, Profit: 1.8189894035458565e-12\n",
      "Episode: 17/100, Profit: -3.637978807091713e-12\n",
      "Episode: 18/100, Profit: 0.0\n",
      "Episode: 19/100, Profit: 0.0\n",
      "Episode: 20/100, Profit: 7.275957614183426e-12\n",
      "Episode: 21/100, Profit: 0.0\n",
      "Episode: 22/100, Profit: -3.637978807091713e-12\n",
      "Episode: 23/100, Profit: -3.637978807091713e-12\n",
      "Episode: 24/100, Profit: -1.8189894035458565e-12\n",
      "Episode: 25/100, Profit: 6.821210263296962e-13\n",
      "Episode: 26/100, Profit: -7.275957614183426e-12\n",
      "Episode: 27/100, Profit: -7.275957614183426e-12\n",
      "Episode: 28/100, Profit: -7.275957614183426e-12\n",
      "Episode: 29/100, Profit: -3.637978807091713e-12\n",
      "Episode: 30/100, Profit: -3.637978807091713e-12\n",
      "Episode: 31/100, Profit: -3.637978807091713e-12\n",
      "Episode: 32/100, Profit: 7.275957614183426e-12\n",
      "Episode: 33/100, Profit: 1.8189894035458565e-12\n",
      "Episode: 34/100, Profit: 7.275957614183426e-12\n",
      "Episode: 35/100, Profit: -3.637978807091713e-12\n",
      "Episode: 36/100, Profit: 6.821210263296962e-13\n",
      "Episode: 37/100, Profit: 0.0\n",
      "Episode: 38/100, Profit: -1.8189894035458565e-12\n",
      "Episode: 39/100, Profit: -3.637978807091713e-12\n",
      "Episode: 40/100, Profit: 0.0\n",
      "Episode: 41/100, Profit: -7.275957614183426e-12\n",
      "Episode: 42/100, Profit: -3.637978807091713e-12\n",
      "Episode: 43/100, Profit: 7.275957614183426e-12\n",
      "Episode: 44/100, Profit: -1.8189894035458565e-12\n",
      "Episode: 45/100, Profit: -3.637978807091713e-12\n",
      "Episode: 46/100, Profit: 0.0\n",
      "Episode: 47/100, Profit: 1.3642420526593924e-12\n",
      "Episode: 48/100, Profit: -3.637978807091713e-12\n",
      "Episode: 49/100, Profit: 0.0\n",
      "Episode: 50/100, Profit: 7.275957614183426e-12\n",
      "Episode: 51/100, Profit: -7.275957614183426e-12\n",
      "Episode: 52/100, Profit: -3.637978807091713e-12\n",
      "Episode: 53/100, Profit: -1.8189894035458565e-12\n",
      "Episode: 54/100, Profit: -3.637978807091713e-12\n",
      "Episode: 55/100, Profit: -3.637978807091713e-12\n",
      "Episode: 56/100, Profit: -7.275957614183426e-12\n",
      "Episode: 57/100, Profit: -3.637978807091713e-12\n",
      "Episode: 58/100, Profit: 7.275957614183426e-12\n",
      "Episode: 59/100, Profit: -1.8189894035458565e-12\n",
      "Episode: 60/100, Profit: -3.637978807091713e-12\n",
      "Episode: 61/100, Profit: -3.637978807091713e-12\n",
      "Episode: 62/100, Profit: -1.8189894035458565e-12\n",
      "Episode: 63/100, Profit: -3.637978807091713e-12\n",
      "Episode: 64/100, Profit: -1.8189894035458565e-12\n",
      "Episode: 65/100, Profit: -3.637978807091713e-12\n",
      "Episode: 66/100, Profit: -3.637978807091713e-12\n",
      "Episode: 67/100, Profit: 0.0\n",
      "Episode: 68/100, Profit: -1.1368683772161603e-12\n",
      "Episode: 69/100, Profit: 1.8189894035458565e-12\n",
      "Episode: 70/100, Profit: -7.275957614183426e-12\n",
      "Episode: 71/100, Profit: 1.8189894035458565e-12\n",
      "Episode: 72/100, Profit: -7.275957614183426e-12\n",
      "Episode: 73/100, Profit: -3.637978807091713e-12\n",
      "Episode: 74/100, Profit: -3.637978807091713e-12\n",
      "Episode: 75/100, Profit: 7.275957614183426e-12\n",
      "Episode: 76/100, Profit: 9.094947017729282e-13\n",
      "Episode: 77/100, Profit: -3.637978807091713e-12\n",
      "Episode: 78/100, Profit: 0.0\n",
      "Episode: 79/100, Profit: 0.0\n",
      "Episode: 80/100, Profit: -3.637978807091713e-12\n",
      "Episode: 81/100, Profit: 0.0\n",
      "Episode: 82/100, Profit: 1.8189894035458565e-12\n",
      "Episode: 83/100, Profit: -9.094947017729282e-13\n",
      "Episode: 84/100, Profit: -1.1368683772161603e-12\n",
      "Episode: 85/100, Profit: -3.637978807091713e-12\n",
      "Episode: 86/100, Profit: -1.1368683772161603e-12\n",
      "Episode: 87/100, Profit: -3.637978807091713e-12\n",
      "Episode: 88/100, Profit: -3.637978807091713e-12\n",
      "Episode: 89/100, Profit: -7.275957614183426e-12\n",
      "Episode: 90/100, Profit: -1.8189894035458565e-12\n",
      "Episode: 91/100, Profit: -3.637978807091713e-12\n",
      "Episode: 92/100, Profit: -3.637978807091713e-12\n",
      "Episode: 93/100, Profit: 0.0\n",
      "Episode: 94/100, Profit: 1.3642420526593924e-12\n",
      "Episode: 95/100, Profit: -3.637978807091713e-12\n",
      "Episode: 96/100, Profit: 1.8189894035458565e-12\n",
      "Episode: 97/100, Profit: -3.637978807091713e-12\n",
      "Episode: 98/100, Profit: 7.275957614183426e-12\n",
      "Episode: 99/100, Profit: 1.8189894035458565e-12\n",
      "Episode: 100/100, Profit: -3.637978807091713e-12\n"
     ]
    }
   ],
   "source": [
    "num_episodes = 100\n",
    "for episode in range(num_episodes):\n",
    "    state = env.reset()\n",
    "    state_index = 0 # This should be more meaningfully defined based on the environment's state \n",
    "\n",
    "    while True:\n",
    "        action = agent.choose_action(state_index)\n",
    "        next_state, reward, done, _ = env.step(action)\n",
    "\n",
    "        # Ensure next_state_indexes doesn't exceed q_table bounds\n",
    "        next_state_index = min(state_index + 1, agent.state_size - 1)\n",
    "        agent.learn(state_index, action, reward, next_state_index, done)\n",
    "\n",
    "        state_index = next_state_index\n",
    "        if done:\n",
    "            break\n",
    "\n",
    "    print(f'Episode: {episode + 1}/{num_episodes}, Profit: {env.cash_in_hand - 1000 + sum(env.positions)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c93bb35e-95c7-40ec-8c7c-51eb62699641",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
